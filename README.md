# üè• RAG-Powered Medical Chatbot

A **production-deployed** Retrieval-Augmented Generation (**RAG**) chatbot, live on **AWS EC2**, that answers medical questions through a full-stack pipeline ‚Äî from a **HTML / CSS / JS / jQuery** frontend, through **Python Flask APIs**, into a **RAG** core powered by **HuggingFace Embeddings** + **Pinecone** vector search + **Groq (LLaMA 3.3)** ‚Äî containerized with **Docker** and shipped automatically via a **CI/CD** pipeline into **AWS ECR ‚Üí EC2**.

> **Stack:** `HTML` ¬∑ `CSS` ¬∑ `JavaScript` ¬∑ `jQuery` ¬∑ `AJAX` ¬∑ `Bootstrap` ¬∑ `Python` ¬∑ `Flask API` ¬∑ `Pinecone` ¬∑ `HuggingFace Embeddings` ¬∑ `Groq (LLaMA 3.3)` ¬∑ ‚≠ê `RAG` ¬∑ `Docker` ¬∑ `CI/CD` ¬∑ `AWS EC2` ¬∑ `AWS ECR`

---

### üîç How This Was Built ‚Äî The Full Story

The frontend is a conversational chat interface built with **HTML, CSS, JavaScript, jQuery, AJAX, and Bootstrap**. When a user types a question and hits send, **AJAX** fires an asynchronous call to a **Flask API** endpoint ‚Äî no page reload, just a smooth chat experience. On the backend, Flask receives the question via the request object and passes it straight into the RAG chain.

The knowledge base powering the chatbot is the **Gale Encyclopedia of Medicine** ‚Äî a comprehensive reference covering virtually every disease, symptom, treatment, and medical concept. This book was loaded as a PDF, parsed into individual documents, and then split into small chunks. Each chunk was passed through a **HuggingFace sentence-transformers embedding model** which converted the text into numerical vectors that capture its semantic meaning. All those vectors were then stored in a **Pinecone index**, turning the encyclopedia into a searchable vector database.

The heart of the project is the **RAG chain** ‚Äî built with LangChain by wiring together three components: a **retriever** (Pinecone similarity search that fetches the top-3 most relevant chunks for any question), an **LLM** (ChatGroq running LLaMA 3.3 70B), and a **prompt template** that instructs the model to answer concisely using only the retrieved context. The chain is fully runnable ‚Äî you pass in a question, you get a grounded answer. If the retrieved context doesn't contain the answer, the model says so rather than guessing.

For deployment, an **AWS ECR** repository was created to store Docker images, and an **AWS EC2** instance was set up to serve the application. AWS access keys were generated, and all sensitive credentials ‚Äî including Pinecone and Groq API keys ‚Äî were stored as **GitHub Secrets**. The EC2 instance was registered as a **self-hosted GitHub Actions runner**, making it the target machine for deployments. A **Dockerfile** packages the Flask app into a portable container, and a **`cicd.yml`** GitHub Actions workflow automates the entire process with two jobs: the **CI job** (runs on a GitHub-hosted runner) builds the Docker image and pushes it to ECR, and the **CD job** (runs on the EC2 self-hosted runner) pulls that image from ECR and runs it on the server. Every `git push` to `main` triggers the pipeline automatically ‚Äî and the chatbot is live and accessible via the EC2 public IP.

---

## üöÄ Live Demo

> Ask the chatbot anything medical ‚Äî symptoms, treatments, drugs, anatomy ‚Äî and it will retrieve the most relevant passages from its knowledge base and generate a concise, grounded answer.

---

## üß† How It Works

```
User Question
     ‚îÇ
     ‚ñº
 Flask App (app.py)
     ‚îÇ
     ‚ñº
 HuggingFace Embeddings          ‚óÑ‚îÄ‚îÄ‚îÄ sentence-transformers/all-MiniLM-L6-v2
 (Convert question to vector)
     ‚îÇ
     ‚ñº
 Pinecone Vector Store           ‚óÑ‚îÄ‚îÄ‚îÄ Pre-indexed medical PDF chunks
 (Similarity search, top-k=3)
     ‚îÇ
     ‚ñº
 Retrieved Context (chunks)
     ‚îÇ
     ‚ñº
 Groq LLM (LLaMA 3.3 70B)       ‚óÑ‚îÄ‚îÄ‚îÄ Generates answer using context
     ‚îÇ
     ‚ñº
 Answer ‚Üí User
```

The chatbot **never hallucinates facts it doesn't have** ‚Äî if the retrieved context doesn't contain the answer, it says so.

---


## üìÅ Directory Structure

```
RAG-Powered-Medical-Chatbot/
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ cicd.yml              # GitHub Actions CI/CD pipeline
‚îÇ
‚îú‚îÄ‚îÄ data/                         
‚îÇ   ‚îî‚îÄ‚îÄ Medical_book.pdf          # Medical PDF: The Gale Encyclopedia of medicine
‚îÇ
‚îú‚îÄ‚îÄ research/
‚îÇ   ‚îî‚îÄ‚îÄ trials.ipynb              # Jupyter notebook for RAG experimentation
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ helper.py                 # PDF loading, chunking, embedding utilities
‚îÇ   ‚îî‚îÄ‚îÄ prompt.py                 # System prompt for the medical assistant
‚îÇ
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îî‚îÄ‚îÄ style.css                 # Chat UI styles
‚îÇ
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html                # Chat UI (HTML + Bootstrap + jQuery)
‚îÇ
‚îú‚îÄ‚îÄ app.py                        # Flask web application (main entry point)
‚îú‚îÄ‚îÄ store_index.py                # One-time script to embed PDFs ‚Üí Pinecone
‚îú‚îÄ‚îÄ Dockerfile                    # Containerization config
‚îú‚îÄ‚îÄ pyproject.toml                # Python dependencies
‚îú‚îÄ‚îÄ template.sh                   # Shell script to scaffold project structure
‚îú‚îÄ‚îÄ uv.lock                       # Lockfile for reproducible installs
‚îú‚îÄ‚îÄ .python-version               # Pinned Python version
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

---

## üõ†Ô∏è Tech Stack

| Layer | Technology |
|---|---|
| **LLM** | Groq API ‚Äî `llama-3.3-70b-versatile` |
| **Embeddings** | `sentence-transformers/all-MiniLM-L6-v2` (HuggingFace) |
| **Vector Store** | Pinecone (Serverless, AWS `us-east-1`) |
| **RAG Framework** | LangChain |
| **Web Framework** | Flask |
| **Frontend** | HTML + CSS + Bootstrap 4 + jQuery |
| **Containerization** | Docker |
| **Cloud** | AWS EC2 + AWS ECR |
| **CI/CD** | GitHub Actions |

---

## ‚öôÔ∏è Setup & Installation

### Prerequisites

- Python >= 3.10
- [uv](https://github.com/astral-sh/uv) (recommended) or pip
- A [Pinecone](https://www.pinecone.io/) account and API key
- A [Groq](https://console.groq.com/) account and API key

### 1. Clone the repository

```bash
git clone https://github.com/manez-github/RAG-Powered-Medical-Chatbot.git
cd RAG-Powered-Medical-Chatbot
```

### 2. Install dependencies

```bash
# Using uv (recommended)
uv sync 
```

### 3. Configure environment variables

Create a `.env` file in the root directory:

```env
PINECONE_API_KEY=your_pinecone_api_key_here
GROQ_API_KEY=your_groq_api_key_here
AWS_ACCESS_KEY_ID=your_aws_key         # Only needed for deployment
AWS_SECRET_ACCESS_KEY=your_aws_secret  # Only needed for deployment
AWS_DEFAULT_REGION=your_aws_region     # Only needed for deployment
```

### 4. Add your medical PDFs

Place your PDF files in the `data/` directory. 

### 5. Index the PDFs into Pinecone

> **Run this only once** (or whenever you add new PDFs to the knowledge base).

```bash
python store_index.py
```

This will:
- Load and parse all PDFs from `data/`
- Split them into 500-token chunks (20-token overlap)
- Generate embeddings using the MiniLM model
- Create a `medical-chatbot` index in Pinecone and upload all vectors

### 6. Run the application

```bash
python app.py
```

The app will be available at `http://localhost:8000`.

---

## üîÑ CI/CD Pipeline

The project uses GitHub Actions for automated build and deployment to AWS EC2.

**Continuous Integration** (runs on GitHub-hosted runner):
1. Checks out the code
2. Authenticates with AWS
3. Builds the Docker image
4. Pushes it to Amazon ECR

**Continuous Deployment** (runs on self-hosted EC2 runner):
1. Pulls the latest image from ECR
2. Stops and removes any existing containers
3. Runs the new container, injecting all secrets as environment variables

### Required GitHub Secrets

| Secret | Description |
|---|---|
| `AWS_ACCESS_KEY_ID` | AWS IAM access key |
| `AWS_SECRET_ACCESS_KEY` | AWS IAM secret key |
| `AWS_DEFAULT_REGION` | AWS region (e.g., `us-east-1`) |
| `ECR_REPO` | ECR repository name |
| `PINECONE_API_KEY` | Pinecone API key |
| `GROQ_API_KEY` | Groq API key |

---

## üî¨ Research & Experimentation

The `research/trials.ipynb` notebook contains exploratory work used to prototype and validate the RAG pipeline before productionizing it ‚Äî including chunk size tuning, embedding evaluation, and retrieval quality checks.

---

## üì¶ Key Dependencies

```toml
langchain==0.3.26
langchain-pinecone==0.2.8
langchain-groq>=0.3.8
langchain-community==0.3.26
sentence-transformers==4.1.0
flask==3.1.1
pypdf==5.6.1
python-dotenv==1.1.0
gunicorn>=25.1.0
```

---

## üóÇÔ∏è Core Module Reference

### `src/helper.py`

| Function | Description |
|---|---|
| `load_pdf_files(path)` | Recursively loads all PDFs from a directory using `PyPDFLoader` |
| `filter_to_minimal_docs(docs)` | Strips metadata down to `source` only for cleaner storage |
| `download_embeddings()` | Downloads and returns the `all-MiniLM-L6-v2` HuggingFace model |
| `create_chunks(docs)` | Splits documents into 500-token chunks with 20-token overlap |

### `src/prompt.py`

Defines the system prompt for the medical assistant. The LLM is instructed to answer concisely using only the retrieved context, and to admit when it doesn't know the answer.

### `store_index.py`

One-time ingestion script. Creates the Pinecone index if it doesn't exist and populates it with vectorized document chunks.

### `app.py`

Flask application. On startup, it connects to the existing Pinecone index and builds the RAG chain. Serves the chat UI at `/` and handles message POST requests at `/get`.

---

## ü§ù Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what you'd like to change.

---

## üìÑ License

This project is open-source. See the repository for license details.